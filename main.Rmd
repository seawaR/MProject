---
title: "Categorical Sequence Analysis with Optimal Matching: An Application with Data from the 'Women 40+ Healthy Aging Study'"
header-includes:
  - \usepackage{algpseudocode}
author: "Adriana Clavijo Daza"
date: "2023-06-02"
output: 
  pdf_document:
    extra_dependencies: 
    - "amsmath"
    - "flafter"
documentclass: report
includes:
      in_header: "preamble.tex"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  message = FALSE,
  warning = FALSE,
  fig.width = 7.5,
  fig.height = 5.25
)
```

```{r libs, echo = FALSE}
library(readxl)
library(tidyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(TraMineR)
library(TraMineRextras)
library(cluster)
source("distance_matrix.R")
source("predict_scores.R")
source("get_MSE.R")
```

```{r, child = "data_prep.Rmd"}
```

```{r, child = "main_analysis.Rmd"}
```

## Optimal Matching

Optimal Matching (OM) is a technique used in social sciences for the comparison of sequences of categorical states indexed by time. this method has applications in different areas of social sciences, for instance, life course or career path analysis. OM uses the Needleman-Wunsch algorithm, that was developed to compare biological sequences. This algorithm is an application of dynamic programming, an iterative method that simplifies an optimization problem by breaking it into a recursion of smaller problems that are simpler to solve.


## The OM algorithm

Consider a set of $n$ categorical states, say, $S = \{s_1, \dots, s_n\}$. A sequence of (discrete) length $t > 0$ can be denoted as $X = (x_1, \dots, x_t)$, where $x_i \in S$ for $i = 1, \dots, t$. Additionally, the set of all possible sequences with states belonging to $S$ is denoted by $\mathbf{S}$.

Now, let $X, Y \in \mathbf{S}$ be two sequences of size $t_X$ and $t_Y$, respectively. In order to assess the similarities between the sequences $X$ and $Y$, and to obtain a numerical value associated with it, we define an empty array, $F$, of size $(t_X+1) \times (t_Y+1)$. 

The algorithm below shows the initialization and recursion to fill the array $F$.
\vspace{12pt}

\begin{algorithmic}[1]
\State $F(1, 1) \gets 0$
\For{$j \gets 2,t_Y+1$}
  \State $F(1,j) \gets F(1, j-1) + d$
\EndFor
\For{$i \gets 2,t_X+1$}
  \State $F(i,1) \gets F(i-1, 1) + d$
\EndFor
\For{$i \gets 2,t_X+1$}
  \For{$j \gets 2,t_Y+1$}
    \State $F(i,j) \gets min\{F(i-1, j)+d, F(i, j-1)+d, F(i-1, j-1)+k(y_{i-1}, x_{j-1})\}$
    \EndFor
\EndFor
\end{algorithmic}

Here, the value $d$ is the cost of inserting a gap in one of the sequences, also known as *indel* cost, and $k(y_{i-1}, x_{j-1})$ is the cost associated to change from the state $y_{i-1}$ to $x_{j-1}$, which is defined in a matrix $K$ of size $n \times n$, commonly known as the cost matrix.

Lines 1-7 of the OM algorithm correspond to initialization. Starting with a cost of 0 in $F(1, 1)$, the first row and column of $F$ represent cumulative costs of successively adding gaps. The remaining lines of the algorithm correspond to the row-wise recursion to fill the array $F$ according to the content of the sequences to be compared: at any step of the recursion, the algorithm is looking at a specific pair of indexes (location) and calculating if substitution or insertion/deletion is the cheapest operation. Successively adding the costs of the cheapest operations results in the overall optimal cost for aligning (arrange to detect similarities) the sequences $X$ and $Y$.

In fact, when $F$ is completely filled, the value in the last cell, i.e. $F(t_X+1, t_Y+1)$ corresponds to the optimal cost of aligning the sequences $X$ and $Y$. It is possible to recover the steps that conduced to this alignment with a traceback from the last cell. However, this is not necessary to obtain the dissimilarities matrix.

The `R` package `TraMineR` provides several functions to analyze and visualize sequential data. In particular, the package implements OM and offers several methods for computing the cost matrix $K$ and the normalization of the dissimilarity matrix.


### Cost matrix

The cost matrix $K$ is a symmetric matrix of size $n \times n$. The value in the $i$-th row and $j$-th column $K(s_i, s_j)$ indicates the cost of moving from state $s_i$ in time $t > 0$ to state $s_j$ in $t+1$.

The following are the methods available in `TraMineR`.


#### Transition rates (`TRATE`):

The substitution cost between states $s_i$ and $s_j$, $1 \leq i, j \leq n$, is calculated as:

\begin{equation}
\label{eq:transition}
K(s_i, s_j) = c - P(s_i|s_j) - P(s_j|s_i),
\end{equation}

where $P(s_i|s_j)$ is the probability of transition from state $s_j$ in time $t$ to $s_i$ in time $t+1$ and $c$ is a constant, set to a value such that $0 \leq K(s_i, s_j) \leq 2$).


#### Chi-squared distance (`FUTURE`):

\begin{equation}
\label{eq:chisq}
K(s_i, s_j) = d_{\chi^2}(\mathbf{P_i}, \mathbf{P_j}),
\end{equation}

where $\mathbf{P_.} = (P(s_1|s_.), \dots, P(s_n|s_.))'$


#### Relative frequencies (`INDELS` and `INDELSLOG`):

\begin{equation}
\label{eq:indels_cost}
K(s_i, s_j) = d_i + d_j,
\end{equation}

where the *indel* cost $d_i$ depends on the state and takes values: 

\begin{align}
g_i &= \frac{1}{f_i}, &\text{for method `INDEL`}, \\
g_i &= \log({\frac{2}{1+f_i}}), &\text{for method `INDELSLOG`}
\end{align}

and $f_i$ is the relative frequency of the state $s_i$ for $i = 1, \dots, n$. 

**Remarks:** 
- For methods `TRATE` and `FUTURE`, the unique *indel* value is $d = max_{1 \leq i,j \leq n} K(i,j)/2$, so that the cost of any transition is always lower or equal than deleting and inserting an element (or vice versa).
- The Needleman-Wunsch algorithm with constant costs for mismatch is known as Levenshtein distance.


### Example 

Let us suppose that $S$ is the alphabet, let $X = \{S,E,N,D\}$ and $Y = \{A,N,D\}$ be two sequences in $\mathbf{S}$. Supposing that $d = 2$ and 

$$
K(i,j) = 
\begin{cases}
0 & \text{if } i=j,\\
3 & \text{otherwise}
\end{cases}
$$
The array $F$ is initialized as follows:

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 |   |   |   |   |
| N | 4 |   |   |   |   |
| D | 6 |   |   |   |   |

To fill  the second row of $F$ we proceed as follows:

- $F(2,2) = min\{F(1, 2)+d, F(2, 1)+d, F(1, 1)+k(y_{1}, x_{1})\} = min\{2+2, 2+2, 0+3\} = 3$
- $F(2,3) = min\{F(1, 3)+d, F(2, 2)+d, F(1, 2)+k(y_{1}, x_{2})\} = min\{4+2, 3+2, 2+3\} = 5$
- $F(2,4) = min\{F(1, 4)+d, F(2, 3)+d, F(1, 3)+k(y_{1}, x_{3})\} = min\{6+2, 5+2, 4+3\} = 7$
- $F(2,5) = min\{F(1, 5)+d, F(2, 4)+d, F(1, 4)+k(y_{1}, x_{4})\} = min\{8+2, 7+2, 6+3\} = 9$
- $F(3,2) = min\{F(2, 2)+d, F(3, 1)+d, F(2, 1)+k(y_{2}, x_{1})\} = min\{3+2, 4+2, 2+3\} = 5$
- $F(3,3) = min\{F(2, 3)+d, F(3, 2)+d, F(2, 2)+k(y_{2}, x_{2})\} = min\{5+2, 5+2, 3+3\} = 6$
- $F(3,4) = min\{F(2, 4)+d, F(3, 3)+d, F(2, 3)+k(y_{2}, x_{3})\} = min\{5+2, 5+2, 5+0\} = 5$
- $F(3,5) = min\{F(2, 4)+d, F(3, 4)+d, F(2, 4)+k(y_{2}, x_{4})\} = min\{7+2, 5+2, 5+3\} = 7$
- $F(4,2) = min\{F(3, 2)+d, F(4, 1)+d, F(3, 1)+k(y_{3}, x_{1})\} = min\{5+2, 6+2, 4+3\} = 7$
- $F(4,3) = min\{F(3, 3)+d, F(4, 2)+d, F(3, 2)+k(y_{3}, x_{2})\} = min\{6+2, 7+2, 5+3\} = 8$
- $F(4,4) = min\{F(3, 4)+d, F(4, 3)+d, F(3, 3)+k(y_{3}, x_{3})\} = min\{5+2, 8+2, 5+3\} = 7$
- $F(4,5) = min\{F(3, 5)+d, F(4, 4)+d, F(3, 4)+k(y_{3}, x_{4})\} = min\{7+2, 7+2, 5+0\} = 5$

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 | 3 | 5 | 7 | 9 |
| N | 4 | 5 | 6 | 5 | 7 |
| D | 6 | 7 | 8 | 7 | 5 |

In this simple example, we can easily obtain two optimal (equivalent) alignments:  
S E N D with  
A - N D or  
- A N D  

In both cases we have two matches (cost 0), one mismatch (cost 3) and one gap (cost 2), giving a total cost 5 that is exactly what we obtained in the last cell of $F$.

The cost of inserting a gap ($d$) is also known as *indel* (insert or delete) cost. In this example we can observe that, in order to obtain sequence $X$ from $Y$ we have to **insert** a term (i.e. insert a gap and then change its value to a specific state). Equivalently, to obtain sequence $Y$ starting from $X$ we have to **delete** one term.


### Normalization

In cases when the lengths of the sequences differ, it can be useful to account for this differences with a normalization factor. 

Given a set two sequences $X, Y \in \mathbf{S}$ of length $t_1$ and $t_2$, respectively. Let $d(X,Y)$ be the distance between the sequences $X$ and $Y$, $t_{max}$ the length of the longest sequence in $\mathbf{S}$ and $d_{max}$ the maximum distance between any pair of sequences in $\mathbf{S}$.

`TraMineR` offers the following options to normalize the distances between sequences:

- `maxlength`:
$$\frac{d(X,Y)}{t_{max}}$$
- `gmean`: 
$$1- \frac{d_{max}-d(X,Y)}{\sqrt{t_1*t_2}}$$
- `maxdist`: 
$$\frac{d(X,Y)}{d_{max}}$$


## Data from the 40+ Healthy Aging Study


### About the data

As part of the Women 40+ Healthy Aging Study, a large study that was conducted by the Department of Clinical Psychology and Psychotherapy of the University of Zurich, a psychometric instrument was developed in order to obtain information about the history of romantic relationships of women. The study was conducted between June 2017 and February 2018 with women between 40 and 75 years who (self-)reported good, very good or excellent health condition and the absence of acute or chronic somatic disease or mental disorder. The participants who reported psychotherapy or psychopharmacological treatment in the previous 6 months were excluded as well as habitual drinkers. Other exclusion criteria were pregnancy in the last 6 months, premature menopause, surgical menopause, intake of hormonal treatment (including contraceptives), shift-work and recent long-distance flight. The participants were recruited from the general population using online advertisement and flyers.

The questionnaire asked the participants to provide information about relationship phases starting from the age of 15 years until the current age at the time of the data collection. The phases were defined by the start and end age and for each phase and information about civil status, relationship status, living situation, children and quality of the relationship was collected. Before including the data corresponding to their own history, the participants were prompted to answer some of the questions based on an example. Some of the participants were excluded when the example entries were not correctly filled. After data cleaning and revisions for consistency the total number of individuals considered is 239.

In order to create a sequence for each participant the information about civil status, relationship status, living situation and the maternity is taken into account. A yearly sequence is created and the states considered are the following:

- 1 = Single + no children
- 2 = Single + children
- 3 = Changing relationships + no children
- 4 = Changing rel. + children
- 5 = Relationship + living apart + no children
- 6 = Relationship + living together + no children 
- 7 = Relationship + living apart + children
- 8 = Relationship + living together + children
- 9 = Married + no children
- 10 = Married + children

Additionally, personality scores for the women included in the study are available. Personality refers to the enduring characteristics and behavior that comprise the unique adjustment to life of a person, including major traits, interests, drives, values, self-concept, abilities, and emotional patterns. These scores are obtained via psychometric instruments and evaluate the main personality traits:

- Agreeableness
- Conscientiousness 
- Extraversion
- Neuroticism
- Openness


### Application of OM

Using the `R` package `TraMineR` the cost matrix is calculated with transition rates between states. We consider a base setup with method `TRATE` for the calculation of the cost matrix and `maxlenght` normalization for the dissimilarities matrix. The obtained cost matrix is shown below.

```{r cost_matrix}
knitr::kable(cost_matrix_base, digits = 2, row.names = FALSE, caption = "Cost matrix obtained from transition probabilities.") %>%
  kableExtra::kable_styling(font_size = 8)
```

As expected, the elements in the diagonal are equal to 0, meaning there is no cost associated to staying in the same state. By default, the constant $c$ in \ref{eq:transition} is set to 2. This, and the fact that the duration of the states is often longer that the time unit (one year), makes that all of the values outside the diagonal are close to 2 and even equal in cases where no transition between the states were observed in the data (e.g. from single without children to single with children and vice versa). Finally, we observe that missing value (`NA`) is considered as a separate state and, by default, the cost of changing from or to a missing value is 2, which might be too high in cases where the individuals made a mistake in the beginning or end age of a phase leaving a gap in the sequence.

From this cost matrix it is possible to calculate pairwise distances between all the sequences using the algorithm described in section XX. As stated before, a correction of the distances is done to account for the differences in size of the sequences. This is done dividing the obtained distance by the length of the longest sequence. 

Having obtained the distance matrix, we apply a hierarchical agglomerative clustering method in order to explore the data and the differences captured by the distance matrix. In particular, we set the number of clusters to 4 and the following figure shows the distribution of the states.

```{r plot-seqdplot-1, out.width = "300px", fig.align = "center", fig.cap = "Distribution of states by cluster."}
# Reduce the bottom and right margin of (sub)plots by ~50%.
par(mar = par()$mar * c(0.5, 1, 1, 0.5))

seqdplot(results$sd,
  group = clusters4_labels,
  border = NA,
  ltext = status_labels
)

invisible(dev.off())
```

The figure below shows the transverse entropy by cluster, i.e. the cross-sectional entropy of the states distributions is calculated at each time point as follows:

\begin{equation}
\label{eq:entropy}
h(f_1, \dots, f_n) = - \sum_{i = 1}^n f_i \log(f_i).
\end{equation}

```{r, out.width = "250px", fig.align = "center", results = "hide", fig.cap = "Transversal entropy by cluster."}
seqplot.tentrop(results$sd, 
                group = clusters4_labels, 
                ylim = c(0, 1),
                main = "")
```

The previous visualizations allow us to try to identify common and contrasting features of the clusters that can be useful to describe them. It is important to remember that this description is subjective and incomplete.

- Cluster 1: Married young and had children.
- Cluster 2: Often in relationships but not married.
- Cluster 3: Older, mostly married or in long relationship without children.
- Cluster 4: Younger, single or in a relationship without children.

```{r, echo=FALSE, message=FALSE, cache=TRUE}
knitr::kable(clusters4_counts, caption = "Number of individuals by cluster.")
```

On the other hand, in figure XX we can also appreciate that the conformation of some clusters seems to be highly affected by the length of the sequence and is possible that the normalization method is not achieving the expected result.

We are interested in exploring how the relationships history of the women relate to personality traits.  As a first exploratory step, the following figure shows the distribution of the score for each trait by cluster. 

```{r, fig.width = 7.5, fig.height = 5.25, out.width = "300px", fig.align = "center", fig.cap = "Distribution of personality scores by cluster."}
p <- ggplot(data_all_long, aes(x = Trait_value, after_stat(density)))
p +
  geom_histogram(bins = 10) +
  facet_grid(
    rows = vars(Cluster_4),
    cols = vars(Trait),
    scales = "free_x"
  ) +
  labs(x = "Trait Value", y = "Density")
```

No difference is obvious at first glance. Also, the number of clusters and the fact that the personality scores are not continuous makes it difficult to appreciate differences. For that reason, we also explore with a lower number of clusters. 

Furthermore, we obtain better defined clusters that are less affected by the length of the sequences as we can observe in the distribution plots of the sequences states: the majority of women in cluster 1 have children, while we mostly find women without children in cluster 2.

```{r plot-seqdplot-2, out.width = "300px", fig.align = "center", fig.cap = "Distribution of states for two clusters."}
seqdplot(results$sd,
  group = clusters2_labels,
  border = NA,
  ltext = status_labels,
  cex.legend = 0.75
)
```

In addition, the transversal entropy of the sequences for the two clusters is displayed in the figure below.

```{r, out.width = "280px", fig.align = "center", results = "hide", fig.cap = "Transversal entropy for two clusters."}
seqplot.tentrop(results$sd, group = clusters2_labels, ylim = c(0, 1))
```

This figure shows that the entropy decreases significantly around mid age for the cluster of women with children as compared to women without children, which means that the variability of the states for the first group is much lower as compared to the second group. This can be interpreted as a sign of stability in the relationship status for women during the time they have children at home.

As before, we  want to explore possible links between the information from the sequences and personality scores. The following figure shows the distribution of the personality traits for the two clusters.

```{r, fig.width = 7.5, fig.height = 5.25, out.width = "300px", fig.align = "center", fig.cap = "Distribution of personality scores for two clusters."}
p <- ggplot(data_all_long, aes(x = Trait_value, after_stat(density)))
p +
  geom_histogram(bins = 10) +
  facet_grid(
    rows = vars(Cluster_2),
    cols = vars(Trait),
    scales = "free_x"
  ) +
  labs(x = "Trait Value", y = "Density")
```

There seems to be differences in the distributions of some personality scores: the scores of agreeableness are concentrated in larger values for women with children; women without children have greater frequency in lower values of conscientiousness than women with children; and women with children exhibit lower scores of neuroticism. 

Even though, the distribution of personality scores by cluster does not reveal significant differences, the obtention of a distance matrix also provide us a numerical expression of the categorical sequences that allows us to use it for other purposes. In particular, we explore the predictive capability of this data with a non-parametric prediction method in the next section.


## Prediction of personality scores with k-Nearest Neighbors

Given a training set $\mathcal{D} = {(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)}$ of $n$ labeled data points, where $x_i \in \mathbb{R}^d$ and $y_i \in \mathcal{Y}$ (a finite set of class labels for classification or a continuous range of values for regression). $k$-NN provides a way to predict the label or value for a new, data point $x_{n+1}$ (for which $Y$ is unknown) by finding the $k$ training data points closest to $x_{n+1}$ and taking a majority vote of their labels (for classification) or averaging the values of $Y$ (for regression).

There are different ways of calculating the distance between the new data point $x_{n+1}$ and the points in $\mathcal{D}$. For instance, the Euclidean o Mahalanobis distances are usually used. In our case we already count with a matrix distance obtained with OM.

The choice of $k$ is a hyperparameter that can be tuned to optimize the performance of the $k$-NN algorithm. A larger $k$ reduces the effect of noise and outliers, but can also lead to overfitting. A smaller $k$ is more sensitive to noise and outliers, but can better capture local structure.

To compare the performance of different values of $k$, we use the mean squared error (MSE).

\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
\end{equation}

where $y_i$ is the observed value and $\hat{Y}_i$ is the predicted valua via $k$NN.

In this part of the analysis we only consider the individuals who have available personality scores, that leaves us with a sample size of 200 individuals. We also split the data into two subsets: train (70%) and test (30%). We evaluate the MSE of the predictions for the individuals in the test set but only using the data from the nearest neighbors available in the train set.

The following figure shows for every personality score and different values of $k$ the MSE, i.e. for $k = 1, \dots, 80$ we predict values of $Y$ and compare them with the observed values using the MSE. As a reference, a red line for every personality trait is added to indicate the MSE of the trivial prediction, i.e. the prediction considering all the sample points in the train set.

For agreeableness, the MSE is minimum around $k=50$ and increases again. However, this minimum is not considerably lower than the trivial prediction.

For openness, it is not very clear where the minimum MSE is and the prediction with kNN is always worse than the trivial prediction.

For conscientiousness, extraversion and neuroticism we observe that the MSE decreases as $k$ increases and takes a minimum value (around $k=15$, $k=10$, $k=5$, respectively) thet is considerably lower than the trivial prediction and then the MSE increases again.

```{r, fig.width = (7.5 * 0.8), fig.height = (5.25 * 1.25),  out.width = "250px", fig.align = "center", fig.cap = "MSE by cluster for base setup."}
base_MSE %>%
  filter(k > 3) %>%
  ggplot() +
  geom_line(aes(x = k, y = MSE)) +
  geom_line(
    aes(x = k, y = Trivial, col = "red"),
    alpha = 0.5,
    show.legend = FALSE
  ) +
  facet_wrap(
    facets = vars(Score),
    ncol = 1,
    scales = "free",
    strip.position = "top"
  ) +
  theme_minimal() +
  theme(strip.text = element_text(hjust = 0))
```

### Other set-ups

- Setting the constant $c$ as the maximum of $2 - P(s_i|s_j) - P(s_j|s_i)$ for the calculation of the cost matrix.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp1a.pdf")
```

- Using `norm = "gmean"` normalization in `TraMineR::seqdist`.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp2a.pdf")
```

- Using `method = "FUTURE"` for the calculation of the cost matrix in `TraMineR::seqcost`.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp3a.pdf")
```

- Using `method = "INDELS"` for the calculation of the cost matrix in `TraMineR::seqcost`.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp4a.pdf")
```

- Using `method = "INDELSLOG"` for the calculation of the cost matrix in `TraMineR::seqcost`.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp5a.pdf")
```

- Using `method = "FUTURE"` for the calculation of the cost matrix in `TraMineR::seqcost` and `norm = "gmean"` normalization in `TraMineR::seqdist`.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp6a.pdf")
```

- Using `method = "FUTURE"` and setting the cost of missing to a fixed value in the cost matrix in `TraMineR::seqcost` and `norm = "gmean"` normalization in `TraMineR::seqdist`.

```{r, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../Output/exp7a.pdf")
```
