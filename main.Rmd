---
title: "Analysis of categorical sequences"
header-includes:
  - \usepackage{algpseudocode}
author: "Adriana Clavijo Daza"
date: "2023-06-02"
output: 
  pdf_document:
    extra_dependencies: ["amsmath"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r libs, echo = FALSE}
library(readxl)
library(tidyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(TraMineR)
library(cluster)
library(FastKNN)
library(TraMineRextras)
source("predict_scores.R")
#source("get_MSE.R")
```

```{r data, echo = FALSE, cache=TRUE}
cs_levels <- 1:6
cs_labels <- c("Single", "Married", "Registered partnership", 
               "Same-sex partnership", "Divorced", "Widowed")

rs_levels <- 1:4
rs_labels <- c("No relationship", "Relationship", 
               "Open relationship", "Changing relationships")

liv_levels <- 0:2
liv_labels <- c("No partner", "Yes", "No")

file <- "../Data_original/Tracker_Uni-Zurich_Modified.xlsx"

raw_data <- read_excel(file)

raw_data <- raw_data %>% 
  select_if(~!all(is.na(.)))

new_names <- vector(mode = "character")

for (i in 1:12){
  new_names[2*(i-1)+1] <- paste0("Start-", i)
  new_names[2*(i-1)+2] <- paste0("End-", i)
}

colnames(raw_data)[1] <- "Id"
colnames(raw_data)[58] <- "Age"
colnames(raw_data)[59:82] <- new_names
colnames(raw_data)[83:94] <- paste0("Civil-", 1:12)
colnames(raw_data)[95:106] <- paste0("Relationship-", 1:12)
colnames(raw_data)[119:130] <- paste0("Children-", 1:12)
colnames(raw_data)[131:142] <- paste0("Living-", 1:12)

raw_data <- raw_data %>% 
  select(c(1, 58:106, 119:142)) 

raw_data <- raw_data %>% 
  pivot_longer(cols = starts_with("Start"),
               names_to = "Phase",
               names_prefix = "Start-",
               values_to = "Start_age",
               values_drop_na = TRUE) %>% 
  pivot_longer(cols = starts_with("End"),
               names_to = "Match2",
               names_prefix = "End-",
               values_to = "End_age",
               values_drop_na = TRUE) %>% 
  pivot_longer(cols = starts_with("Civil"),
               names_to = "Match3",
               names_prefix = "Civil-",
               values_to = "Civil_status",
               values_drop_na = TRUE) %>% 
  pivot_longer(cols = starts_with("Relationship"),
               names_to = "Match4",
               names_prefix = "Relationship-",
               values_to = "Relationship_status",
               values_drop_na = TRUE) %>% 
  pivot_longer(cols = starts_with("Children"),
               names_to = "Match5",
               names_prefix = "Children-",
               values_to = "Children",
               values_drop_na = TRUE) %>% 
  pivot_longer(cols = starts_with("Living"),
               names_to = "Match6",
               names_prefix = "Living-",
               values_to = "Living_situation",
               values_drop_na = TRUE) %>% 
  filter(Phase == Match2,
         Phase == Match3,
         Phase == Match4,
         Phase == Match5,
         Phase == Match6,
         Start_age >= 15,
         !Id %in% c("zr34u", "EN61O", "DN15U"),
         !(Id == "GB28U" & Phase == "4")) %>% 
  select(-contains("Match")) %>% 
  mutate(Civil_status = factor(Civil_status, levels = cs_levels, labels = cs_labels),
         Relationship_status = factor(Relationship_status, levels = rs_levels, labels = rs_labels),
         Phase = as.numeric(Phase),
         Living_situation = factor(Living_situation, levels = liv_levels, labels = liv_labels),
         Children = case_when(
           Children == 0 ~ "No",
           is.na(Children) ~ "No",
           Children > 0 ~ "Yes"
         )
         )

tst <- raw_data %>% 
  mutate(
    Status= case_when(
      Relationship_status == "No relationship" & Civil_status == "Single" & Children == "No" ~ 1,
      Relationship_status == "No relationship" & Civil_status == "Single" & Children == "Yes" ~ 2,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Single" & Children == "No" & Living_situation == "No" ~ 5,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Single" & Children == "No" & Living_situation == "Yes"  ~ 6,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Single" & Children == "Yes" & Living_situation == "No" ~ 7,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Single" & Children == "Yes"  & Living_situation == "Yes" ~ 8,
      Relationship_status == "Changing relationships" & Civil_status == "Single" & Children == "No" ~ 3,
      Relationship_status == "Changing relationships" & Civil_status == "Single" & Children == "Yes" ~ 4,
      Civil_status %in% c("Married", "Registered partnership") & Children == "No" ~ 9,
      Civil_status %in% c("Married", "Registered partnership") & Children == "Yes" ~ 10,
      Relationship_status == "No relationship" & Civil_status == "Divorced" & Children == "No" ~ 1,
      Relationship_status == "No relationship" & Civil_status == "Divorced" & Children == "Yes" ~ 5,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Divorced" & Children == "No" & Living_situation == "No" ~ 5,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Divorced" & Children == "No" & Living_situation == "Yes" ~ 6,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Divorced" & Children == "Yes" & Living_situation == "No" ~ 7,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Divorced" & Children == "Yes" & Living_situation == "Yes" ~ 8,
      Relationship_status == "Changing relationships" & Civil_status == "Divorced" & Children == "No" ~ 3,
      Relationship_status == "Changing relationships" & Civil_status == "Divorced" & Children == "Yes" ~ 4,
      Relationship_status == "No relationship" & Civil_status == "Widowed" & Children == "No" ~ 1,
      Relationship_status == "No relationship" & Civil_status == "Widowed" & Children == "Yes" ~ 2,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Widowed" & Children == "No" & Living_situation == "No" ~ 5,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Widowed" & Children == "No" & Living_situation == "Yes" ~ 6,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Widowed" & Children == "Yes" & Living_situation == "No" ~ 7,
      Relationship_status %in% c("Relationship", "Open relationship") & Civil_status == "Widowed" & Children == "Yes" & Living_situation == "Yes" ~ 8,
      Relationship_status == "Changing relationships" & Civil_status == "Widowed" & Children == "No" ~ 3,
      Relationship_status == "Changing relationships" & Civil_status == "Widowed" & Children == "Yes" ~ 4,
      TRUE ~ 0
      )
    )                

status_levels <- 1:10
status_labels <- c("Single+no ch.",
                   "Single+ch.",
                   "Changing rel.+no ch.",
                   "Changing rel.+ch.",
                   "Rel.+apart+no ch.",
                   "Rel.+together+no ch.",
                   "Rel.+apart+ch.",
                   "Rel.+together+ch.",
                   "Married+no ch.",
                   "Married+ch.")

rh_data <- tst %>% 
  select(-Civil_status, -Relationship_status, -Children) %>% 
  mutate(Status_char = factor(Status, levels = status_levels, labels = status_labels))
```


## Optimal Matching

Optimal Matching (OM) is a technique used in social sciences for the comparison of sequences of categorical states indexed by time. this method has applications in different areas of social sciences, for instance, life course or career path analysis. OM uses the Needleman-Wunsch algorithm, that was developed to compare biological sequences. This algorithm is an application of dynamic programming, an iterative method that simplifies an optimization problem by breaking it into a recursion of smaller problems that are simpler to solve.

## The OM algorithm

Given a set of $n$ states, say, $S = \{s_1, \dots, s_n\}$ a sequence of size $t > 0$ can be denoted as $X = (x_1, \dots, x_t)$, where $x_i \in S$ for $i = 1, \dots, t$. Additionally, the set of all possible sequences with states belonging to $S$ is denoted by $\mathbf{S}$.

Now, let $X, Y \in \mathbf{S}$ be two sequences of size $t_1$ and $t_2$, respectively. In order to find the optimal way to align these two sequences, we define an empty array, $F$, of size $(t_1+1) \times (t_2+1)$. The algorithm below explains how the array $F$ is filled.
\vspace{12pt}

\begin{algorithmic}[1]
\State $F(1, 1) \gets 0$
\For{$j \gets 2,t_2+1$}
  \State $F(1,j) \gets F(1, j-1) + d$
\EndFor
\For{$i \gets 2,t_1+1$}
  \State $F(i,1) \gets F(i-1, 1) + d$
\EndFor
\For{$i \gets 2,t_1+1$}
  \For{$j \gets 2,t_2+1$}
    \State $F(i,j) \gets min\{F(i-1, j)+d, F(i, j-1)+d, F(i-1, j-1)+k(y_{i-1}, x_{j-1})\}$
    \EndFor
\EndFor
\end{algorithmic}

Here, $d$ is the cost of inserting a gap, and $k(y_{i-1}, x_{j-1})$ is the cost associated to change from the state $y_{i-1}$ to $x_{j-1}$, which is defined in a matrix $K$ of size $n \times n$, usually known as the cost matrix.

The cost of inserting a gap, $d$, to align the sequences is also known as INDEL (insert or delete) cost and it can also be interpreted as deleting a term of the sequence $X$ or adding a term to the sequence $Y$, which is equivalent.

Lines 1-7 of the algorithm correspond to initialization and equation; starting with a cost of 0 in $F(1, 1)$ and with the first row and column representing cumulative costs of successively adding gaps. The remaining lines of the algorithm correspond to the row-wise recursion to fill the array $F$ according to the content of the sequences to be aligned. At any step of the recursion, the algorithm is looking at a specific pair of indexes (location) and calculating if substitution or insertion/deletion is the cheapest operation. Successively adding the costs of the cheapest operations results in the overall optimal cost for aligning the sequences $X$ and $Y$.

When $F$ is completely filled, the value in the last cell, i.e. $F(t_1+1, t_2+1)$ corresponds to the optimal cost of aligning the sequences $X$ and $Y$. It is possible to recover the steps with a traceback from the last cell. However, this is not necessary to perform OM.

### Cost matrix

The `R` package `TraMineR` provides several functions to work with sets of sequences. The package implements OM and offers several methods for computing the cost matrix $K$.

#### Transition rates (`TRATE`)

The substitution cost between states $s_i$ and $s_j$, $1 \leq i, j \leq n$, is calculated as:

\begin{equation}
\label{eq:transition}
K(s_i, s_j) = c - P(s_i|s_j) - P(s_j|s_i),
\end{equation}

where $P(s_i|s_j)$ is the probability of transition from state $s_j$ in time $t$ to $s_i$ in time $t+1$ and $c$ is a constant, set to a value such that $0 \leq K(s_i, s_j) \leq 2$).


### Chi-squared distance (`FUTURE`)

\begin{equation}
\label{eq:chisq}
K(s_i, s_j) = ChiDist(\mathbf{P_i}, \mathbf{P_j}),
\end{equation}

where $\mathbf{P_.} = (P(s_1|.), \dots, P(s_n|.))'$


### Relative frequencies (`INDELS` and `INDELSLOG`)

\begin{equation}
\label{eq:indels}
K(s_i, s_j) = indel_i + indel_j,
\end{equation}

where $indel_i = 1/f_i$ for method `INDEL`, $indel_i = \log[2/(1+f_i)]$ and $f_i$ is the relative frequency of the state $s_i$ for $i = 1, \dots, n$. 

### Example 

Let us suppose that $S$ is the alphabet, let $X = \{S,E,N,D\}$ and $Y = \{A,N,D\}$ be two sequences in $\mathbf{S}$. Supposing that $d = 2$ and 

$$
K(i,j) = 
\begin{cases}
0 & \text{if } i=j,\\
3 & \text{otherwise}
\end{cases}
$$
The array $F$ is initialized as follows:

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 |   |   |   |   |
| N | 4 |   |   |   |   |
| D | 6 |   |   |   |   |

To fill  the second row of $F$ we proceed as follows:

- $F(2,2) = min\{F(1, 2)+d, F(2, 1)+d, F(1, 1)+k(y_{1}, x_{1})\} = min\{2+2, 2+2, 0+3\} = 3$
- $F(2,3) = min\{F(1, 3)+d, F(2, 2)+d, F(1, 2)+k(y_{1}, x_{2})\} = min\{4+2, 3+2, 2+3\} = 5$
- $F(2,4) = min\{F(1, 4)+d, F(2, 3)+d, F(1, 3)+k(y_{1}, x_{3})\} = min\{6+2, 5+2, 4+3\} = 7$
- $F(2,5) = min\{F(1, 5)+d, F(2, 4)+d, F(1, 4)+k(y_{1}, x_{4})\} = min\{8+2, 7+2, 6+3\} = 9$
- $F(3,2) = min\{F(2, 2)+d, F(3, 1)+d, F(2, 1)+k(y_{2}, x_{1})\} = min\{3+2, 4+2, 2+3\} = 5$
- $F(3,3) = min\{F(2, 3)+d, F(3, 2)+d, F(2, 2)+k(y_{2}, x_{2})\} = min\{5+2, 5+2, 3+3\} = 6$
- $F(3,4) = min\{F(2, 4)+d, F(3, 3)+d, F(2, 3)+k(y_{2}, x_{3})\} = min\{5+2, 5+2, 5+0\} = 5$
- $F(3,5) = min\{F(2, 4)+d, F(3, 4)+d, F(2, 4)+k(y_{2}, x_{4})\} = min\{7+2, 5+2, 5+3\} = 7$
- $F(4,2) = min\{F(3, 2)+d, F(4, 1)+d, F(3, 1)+k(y_{3}, x_{1})\} = min\{5+2, 6+2, 4+3\} = 7$
- $F(4,3) = min\{F(3, 3)+d, F(4, 2)+d, F(3, 2)+k(y_{3}, x_{2})\} = min\{6+2, 7+2, 5+3\} = 8$
- $F(4,4) = min\{F(3, 4)+d, F(4, 3)+d, F(3, 3)+k(y_{3}, x_{3})\} = min\{5+2, 8+2, 5+3\} = 7$
- $F(4,5) = min\{F(3, 5)+d, F(4, 4)+d, F(3, 4)+k(y_{3}, x_{4})\} = min\{7+2, 7+2, 5+0\} = 5$

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 | 3 | 5 | 7 | 9 |
| N | 4 | 5 | 6 | 5 | 7 |
| D | 6 | 7 | 8 | 7 | 5 |

In this simple example, we can easily obtain two optimal (equivalent) alignments:  
S E N D with  
A - N D or  
- A N D  

In both cases we have two matches (cost 0), one mismatch (cost 3) and one gap (cost 2), giving a total cost 5 that is exactly what we obtained in the last cell of $F$.

### Normalization

For cases when we have sequences of different lengths, it can be useful to account for this differences with a normalization factor. 

Given a set two sequences $X, Y \in \mathbf{S}$ of length $t_1$ and $t_2$, respectively. Let $d_{X,Y}$ be the distance between the sequences $X$ and $Y$, $t_{max}$ the length of the longest sequence in $\mathbf{S}$ and $d_{max}$ the maximum distance between any pair of sequences in $\mathbf{S}$.

We can find the following options to normalize the distances between sequences:

- `maxlength`: $d_{X,Y}/t_{max}$
- `gmean`: $1- \frac{d_{max}-d_{X,Y}}{\sqrt{t_1*t_2}}$
- `maxdist`: $d_{X,Y}/d_{max}$

## Data from the 40+ Healthy Aging Study

### About the data

As part of the Women 40+ Healthy Aging Study, a large study that was conducted by the Department of Clinical Psychology and Psychotherapy of the University of Zurich, a psychometric instrument was developed in order to obtain information about the history of romantic relationships of women. The study was conducted between June 2017 and February 2018 with women between 40 and 75 years who (self-)reported good, very good or excellent health condition and the absence of acute or chronic somatic disease or mental disorder. The participants who reported psychotherapy or psychopharmacological treatment in the previous 6 months were excluded as well as habitual drinkers. Other exclusion criteria were pregnancy in the last 6 months, premature menopause, surgical menopause, intake of hormonal treatment (including contraceptives), shift-work and recent long-distance flight. The participants were recruited from the general population using online advertisement and flyers.

The questionnaire asked the participants to provide information about relationship phases starting from the age of 15 years until the current age at the time of the data collection. The phases were defined by the start and end age and for each phase and information about civil status, relationship status, living situation, children and quality of the relationship was collected. Before including the data corresponding to their own history, the participants were prompted to answer some of the questions based on an example. Some of the participants were excluded when the example entries were not correctly filled. In total 250 individuals were considered in the analysis.

In order to create a sequence for each participant the information about civil status, relationship status, living situation and the matenity is taken into account. A yearly sequence is created and the states considered are the following:

- 1 = Single + no children
- 2 = Single + children
- 3 = Changing relationships + no children
- 4 = Changing rel. + children
- 5 = Relationship + living apart + no children
- 6 = Relationship + living together + no children 
- 7 = Relationship + living apart + children
- 8 = Relationship + living together + children
- 9 = Married + no children
- 10 = Married + children

Additionally, personality scores for the women included in the study are available. Personality refers to the enduring characteristics and behavior that comprise a person’s unique adjustment to life, including major traits, interests, drives, values, self-concept, abilities, and emotional patterns. These scores are obtained via psychometric instruments and evaluate the main personality traits:

- Agreeableness
- Conscientiousness 
- Extraversion
- Neuroticism
- Openness

Optimal matching analysis is performed with the aim to obtain clusters of sequences that are similar and characterize the most common relationship history profiles.

### Application of OM

Using the `R` package `TraMineR` the cost matrix is calculated with transition rates between states.

```{r cost-matrix, echo=FALSE, message=FALSE}
test <- seqformat(rh_data, from = "SPELL", to = "STS",
                  id = "Id", begin = "Start_age", end = "End_age", 
                  status = "Status", covar = "Age", process = FALSE)

alphabet <- as.character(1:10)

my_seq <- seqdef(test, alphabet = alphabet)

cost_matrix_1 <- seqsubm(my_seq, method = "TRATE", with.missing = TRUE)

cm <- cbind(1:10, data.frame(cost_matrix_1[1:10,1:10]))
colnames(cm) <- c("Status", 1:10)
knitr::kable(cm, digits = 4)
```

From this cost matrix it is possible to calculate pairwise distances between sequences using the algorithm previously described. A correction of the distances is done to account for the differences in size of the sequences. This is done dividing the obtained distance by the length of the longest sequence.

We then use this sequences to apply a hierarchical agglomerative clustering method called AGNES. The following figure shows the dendrogram. In this case, we decided to cut at 5 clusters in order to preserve enough individuals in each cluster.

```{r, echo=FALSE, message=FALSE}
my_dist <- seqdist(my_seq, method = "OM", sm = cost_matrix_1, 
                   with.missing = TRUE, norm = "maxlength")
```

```{r, echo=FALSE, message=FALSE}
clusterward <- agnes(my_dist, diss = TRUE, method = "ward")
```

```{r, out.width="400px", fig.align="center"}
plot(clusterward, which.plots = 2, main = "Dendrogram")
abline(h = 4.3, col = "red")
```

```{r, echo=FALSE, message=FALSE, cache=TRUE}
clusters5 <- cutree(clusterward, k = 5)

clusters5_labels <- factor(clusters5, labels = paste("Cluster", 1:5))

counts <- tibble(clusters5) %>% 
  group_by(clusters5) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  rename(Cluster = clusters5)

knitr::kable(counts)
```

We can visualize the clusters of sequences to and try to identify common features to describe them. It is important to consider that this description is subjective but can be useful to characterize the groups.

- Cluster 1: Married with children then divorced/widowed
- Cluster 2: Sequences with more changes (unstable)
- Cluster 3: Younger, mostly not married without children
- Cluster 4: Older, without children
- Cluster 5: Married late then divorced/widowed, without children

```{r, out.width = "370px", fig.align = "center"}
#knitr::include_graphics("../Output/cluster2.png")
par(mar = c(2, 1.7, 0.75, 0.5))
seqdplot(my_seq, group = clusters5_labels, border = NA, 
         ltext = status_labels)
invisible(dev.off())
```

```{r, echo=FALSE, message=FALSE}
BFI_data <- read_excel("../Data_original/Adriana_BFI.xlsx")

BFI_data <- BFI_data %>% 
  select(PersCode,
         starts_with("BFI"))

pers_descrip <- BFI_data %>% 
  select(PersCode,
         ends_with("mean")) %>% 
  pivot_longer(cols = ends_with("mean"), 
               names_to = "Personality trait", 
               values_to = "Value") %>% 
  group_by(`Personality trait`) %>% 
  summarise(Min = min(Value, na.rm = TRUE),
            Max = max(Value, na.rm = TRUE),
            Average = mean(Value, na.rm = TRUE),
            `Std. deviation` = sd(Value, na.rm = TRUE)) %>% 
  ungroup()

pers_descrip[, 1] <- c("Agreeableness", "Conscientiousness",
                       "Extraversion", "Neuroticism", "Openness")

# knitr::kable(pers_descrip, digits = 2) %>% 
#   kableExtra::kable_styling(font_size = 8)
```

```{r, echo=FALSE, message=FALSE, cache=TRUE}
clusters2 <- cutree(clusterward, k = 2)

clusters2_labels <- factor(clusters2, labels = paste("Cluster", 1:2))
```

```{r, echo=FALSE, message=FALSE}
data_all <- tibble(Id = colnames(my_dist), Cluster_5 = clusters5_labels,
                   Cluster_2 = clusters2_labels) %>% 
  left_join(BFI_data, by = c("Id" = "PersCode")) %>% 
  select(Id, Cluster_5, Cluster_2, ends_with("mean")) %>% 
  filter(!is.na(BFI_extraversion_mean))

data_all_long <- data_all %>% 
  pivot_longer(cols = starts_with("BFI"), names_to = "Trait", 
               names_prefix = "BFI_", values_to = "Trait_value") %>% 
  mutate(Trait = case_when(Trait == "extraversion_mean" ~ "Extraversion",
                           Trait == "agreeableness_mean" ~ "Agreeableness",
                           Trait == "conscientiousness_mean" ~ "Conscientiousness",
                           Trait == "neuroticism_mean" ~ "Neuroticism",
                           Trait == "openness_mean" ~ "Openness",
                           TRUE ~ "other"),
         Cluster_5 = sub("Cluster ", "", Cluster_5),
         Cluster_2 = sub("Cluster ", "", Cluster_2))

# by_cluster <- data_all %>% 
#   group_by(Cluster) %>% 
#   summarise(Extraversion = mean(BFI_extraversion_mean, na.rm = TRUE),
#             Agreeableness = mean(BFI_agreeableness_mean, 
#                                          na.rm = TRUE),
#             Conscientiousness = mean(BFI_conscientiousness_mean,
#                                              na.rm = TRUE),
#             Neuroticism = mean(BFI_neuroticism_mean, na.rm = TRUE),
#             Openness = mean(BFI_openness_mean, na.rm = TRUE)) %>% 
#   ungroup()
# 
# knitr::kable(by_cluster, digits = 2) %>% 
#   kableExtra::kable_styling(font_size = 7)
```

Now, we are interested in predicting the personality scores based on the relationships history of the women. The following figure shows the distribution of the score for each trait by cluster. 

```{r, out.width="350px", fig.align="center"}
p <- ggplot(data_all_long, aes(x = Trait_value, after_stat(density))) 
p + geom_histogram(bins = 12) + facet_grid(rows = vars(Cluster_5), 
                                           cols = vars(Trait),
                                           scales = "free_x")
```

No difference is obvious at a first glance. However, we can also use the distance matrix to obtain predictions of the personality traits using $k$-nearest neighbors ($k$NN); a non-parametric method used for prediction.

We also explore with a lower number of clusters in order to find better defined groups. As we can observe in the distribution plots of the sequences states, the majority of women in cluster 1 have children, while we mostly find women without children in cluster 2.

```{r, echo=FALSE, message=FALSE, cache=TRUE}
counts2 <- tibble(clusters2) %>% 
  group_by(clusters2) %>% 
  summarise(n = n()) %>% 
  ungroup() %>% 
  rename(Cluster = clusters2)

knitr::kable(counts2)
```

```{r, out.width = "350px", fig.align = "center"}
seqdplot(my_seq, group = clusters2_labels, border = NA, 
         ltext = status_labels, with.legend = "right")
invisible(dev.off())
```

The transversal entropy of the sequences by cluster is displayed in the figure below.

```{r, out.width="350px", fig.align="center"}
seqplot.tentrop(my_seq, group = clusters2_labels, ylim=c(0,1))
```

We can observe that the entropy decreases significantly for the cluster of women with children as compared to women without children, which means that the variability of the states for the first group. This can be interpreted as a sign of stability.

The following figure shows the distribution of the personality traits for the two clusters.

```{r, out.width="350px", fig.align="center"}
p <- ggplot(data_all_long, aes(x = Trait_value, after_stat(density))) 
p + geom_histogram(bins = 10) + facet_grid(rows = vars(Cluster_2), 
                                           cols = vars(Trait),
                                           scales = "free_x")
```

There seems to be differences in the distributions of agreeableness, conscientiousness and neuroticism between the clusters. In the following section we explore the predictive capacity of the distance matrix obtained via OM.

## k-Nearest Neighbors

Given a training set $\mathcal{D} = {(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)}$ of $n$ labeled data points, where $x_i \in \mathbb{R}^d$ and $y_i \in \mathcal{Y}$ (a finite set of class labels for classification or a continuous range of values for regression). $k$-NN provides a way to predict the label or value for a new, data point $x_{n+1}$ (for which $Y$ is unknown) by finding the $k$ training data points closest to $x_{n+1}$ and taking a majority vote of their labels (for classification) or averaging the values of $Y$ (for regression).

There are different ways of calculating the distance between the new data point $x_{n+1}$ and the points in $\mathcal{D}$. For instance, the Euclidean o Mahalanobis distances are usually used. In our case we already count with a matrix distance obtained with OM.

The choice of $k$ is a hyperparameter that can be tuned to optimize the performance of the $k$-NN algorithm. A larger $k$ reduces the effect of noise and outliers, but can also lead to overfitting. A smaller $k$ is more sensitive to noise and outliers, but can better capture local structure.

To compare the performance of different values of $k$, we use the mean squared error (MSE).

\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{Y}_i)^2
\end{equation}

where $y_i$ is the observed value and $\hat{Y}_i$ is the predicted valua via $k$NN.

In this part of the analysis we only consider the individuals who have available personality scores, that leaves us with a sample size of 200 individuals. We also split the data into two subsets: train (70%) and test (30%). We evaluate the MSE of the predictions for the individuals in the test set but only using the data from the nearest neighbors available in the train set.

The following figure shows for every personality score and different values of $k$ the MSE, i.e. for $k = 1, \dots, 80$ we predict values of $Y$ and compare them with the observed values using the MSE. As a reference, a red line for every personality trait is added to indicate the MSE of the trivial prediction, i.e. the prediction considering all the sample points in the train set.

For agreeableness, the MSE is minimum around $k=50$ and increases again. However, this minimum is not considerably lower than the trivial prediction.

For openness, it is not very clear where the minimum MSE is and the prediction with kNN is always worse than the trivial prediction.

For conscientiousness, extraversion and neuroticism we observe that the MSE decreases as $k$ increases and takes a minimum value (around $k=15$, $k=10$, $k=5$, respectively) thet is considerably lower than the trivial prediction and then the MSE increases again.

```{r, echo=FALSE, message=FALSE, cache=TRUE}
set.seed(123)
n <- dim(data_all)[1]
size <- round(n*0.3)
max_neighbors <- 80

test_set <- sample.int(n, size)
train_set <- (1:n)[-test_set]

test_names <- colnames(my_dist)[test_set]
train_names <- colnames(my_dist)[train_set]

predict_scores <- function(data, neighbors){
  predictions <- data %>% 
    filter(Id %in% neighbors) %>% 
    summarise(Extraversion = mean(BFI_extraversion_mean, na.rm = TRUE),
              Agreeableness = mean(BFI_agreeableness_mean, na.rm = TRUE),
              Conscientiousness = mean(BFI_conscientiousness_mean, na.rm = TRUE),
              Neuroticism = mean(BFI_neuroticism_mean, na.rm = TRUE),
              Openness = mean(BFI_openness_mean, na.rm = TRUE)) %>% 
    as.vector() %>% 
    unlist()
  return(predictions)
}

results_test <- list()

for(k in 1:max_neighbors){
all_neighbors_test <- list()

for(i in seq_along(test_names)){
  idx <- c(test_names[i], train_names)
  neighbors <- k.nearest.neighbors(i = 1, 
                                   distance_matrix = my_dist[idx, idx], 
                                   k = k) - 1
  all_neighbors_test[[i]] <- train_names[neighbors]
}

names(all_neighbors_test) <- test_names

all_predictions_test <- list()

for(i in seq_along(test_names)){
  all_predictions_test[[i]] <- predict_scores(data_all, neighbors = all_neighbors_test[[i]])
}

names(all_predictions_test) <- test_names

predictions_test_df <- do.call(rbind.data.frame, all_predictions_test)

colnames(predictions_test_df) <- names(all_predictions_test[[1]])

predictions_test_df[, "Id"] <- test_names

sq_error_test <- predictions_test_df %>% 
  left_join(data_all, by = "Id") %>% 
  mutate(sq_diff_Extraversion = (Extraversion - BFI_extraversion_mean)^2,
         sq_diff_Agreeableness = (Agreeableness - BFI_agreeableness_mean)^2,
         sq_diff_Conscientiousness = (Conscientiousness - BFI_conscientiousness_mean)^2,
         sq_diff_Neuroticism = (Neuroticism - BFI_neuroticism_mean)^2,
         sq_diff_Openness = (Openness - BFI_openness_mean)^2) %>% 
  select(Id, Cluster_5, Cluster_2, starts_with("sq_diff")) %>% 
  pivot_longer(starts_with("sq_diff"), names_to = "Score", values_to = "Value") %>% 
  group_by(Score) %>% 
  summarise(MSQ = mean(Value, na.rm = TRUE)) %>% 
  ungroup()

colnames(sq_error_test)[2] <- paste0("MSQ_", k)

sq_error_test[, 1] <- c("Agreeableness", "Conscientiousness",
                        "Extraversion", "Neuroticism", "Openness")

results_test[[k]] <- sq_error_test
}

test_MSQ <- results_test %>% 
  purrr::reduce(left_join, by = "Score") %>% 
  pivot_longer(starts_with("MSQ_"), names_prefix = "MSQ_", names_to = "k", 
               values_to = "MSE") %>% 
  mutate(k = as.numeric(k))
```

```{r, echo=FALSE, message=FALSE, out.width = "400px", fig.align = "center"}
ggplot(data = test_MSQ, aes(x = k, y = MSE, col = Score)) +
  geom_line() + ylim(0.3, 1)
```

```{r}
train_means <- data_all_long %>% 
  filter(Id %in% train_names) %>% 
  rename(Score = Trait) %>% 
  group_by(Score) %>% 
  summarise(Average = mean(Trait_value)) %>% 
  ungroup()

trivial_MSE <- data_all_long %>% 
  rename(Score = Trait) %>% 
  filter(Id %in% test_names) %>% 
  left_join(train_means, by = "Score") %>% 
  mutate(sq_difference = (Trait_value - Average)^2) %>%
  group_by(Score) %>% 
  summarise(MSE = mean(sq_difference)) %>% 
  ungroup()
  
ggplot(test_MSQ %>% filter(k>5)) + 
  geom_line(aes(x = k, y = MSE)) +
  geom_hline(data = trivial_MSE, 
             aes(yintercept = MSE, col = "red"), 
             alpha = 0.5,
             show.legend = FALSE) +
  facet_grid(rows = vars(Score), scales = "free")
```


### Other experiments

- Setting the constant $c$ as the maximum of $2 - P(s_i|s_j) - P(s_j|s_i)$ for the calculation of the cost matrix.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp1a.pdf")
```

- Using `norm = "gmean"` normalization in `TraMineR::seqdist`.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp2a.pdf")
```

- Using `method = "FUTURE"` for the calculation of the cost matrix in `TraMineR::seqcost`.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp3a.pdf")
```

- Using `method = "INDELS"` for the calculation of the cost matrix in `TraMineR::seqcost`.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp4a.pdf")
```

- Using `method = "INDELSLOG"` for the calculation of the cost matrix in `TraMineR::seqcost`.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp5a.pdf")
```

- Using `method = "FUTURE"` for the calculation of the cost matrix in `TraMineR::seqcost` and `norm = "gmean"` normalization in `TraMineR::seqdist`.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp6a.pdf")
```

- Using `method = "FUTURE"` and setting the cost of missing to a fixed value in the cost matrix in `TraMineR::seqcost` and `norm = "gmean"` normalization in `TraMineR::seqdist`.

```{r, out.width = "400px", fig.align = "center"}
knitr::include_graphics("../Output/exp7a.pdf")
```