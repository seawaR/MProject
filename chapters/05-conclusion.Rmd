# Conclusion {.unnumbered}

The use of optimal matching to calculate distances for categorical sequences in the context of social studies has been extensively documented. Many of these applications use the distance matrix obtained via OM as an input for an unsupervised learning technique, such as clustering or multidimensional scaling. Furthermore, this exercise implies a number of considerations in the calculation of the cost matrix and normalization of the distances. We consider these decisions and the ones concerning the handling of missing values as hyperparameters that are subject to tuning.

With a dataset of categorical sequences that contain information about the relationship and family history of a group of women we performed OM and obtained clusters that allow us to identify the most usual trajectories.dditionally, we explored the potential to use the resulting distance matrix for the prediction of personality traits scores with $k$-NN; note that we are adding another hyperparameter to our analysis, namely $k$. We decided to use MSE to evaluate the performance of the predictions under different scenarios, i.e. combinations of hyperparameters. 

Based on the analysis of MSE obtained in different scenarios, we have determined that there is not a single specific combination of cost matrix generation method, normalization method, and treatment of missing values that consistently yields the best prediction for personality scores. However, when it comes to most personality traits, it is preferable to use the cost matrix calculated with the $\chi^2$-distance of the states frequencies. 

We also noted that the difference in sequence lengths affected the performance of the prediction as this produced a large amount of missing values, showing the limitation of any normalization method used.

Furthermore, we also observed that in the two scenarios that we considered for clustering, the resulting states distributions when dividing the individuals into two clusters, show that the main differentiating factor between them is the presence or absence of children (see Figure \@ref(fig:seqdplot-cluster-b) and Figure \@ref(fig:plot-seqdplot-fin)).

It is worth noting that critics of this technique have pointed out that the results heavily rely on how the cost matrix is defined and we have confirmed this with the results of the prediction performance. However, when it comes to unsupervised learning, the results seem to be more robust, except for methods `INDELS` and `INDELSLOG` that produced unidentifiable clusters.

As for future work, we consider that it would be convenient to consider another method for obtaining the cost matrix. For instance, in the specific context of the relationship history, it makes sense to consider an asymmetrical cost matrix as it is likely that the change from one state to another is more frequent that the converse. We also recommend a cross-validation approach for a more robust comparison of the scenarios. Additionally, it would be ideal to count with an objective measure for the comparison of the clusters obtained and consider other techniques for unsupervised learning. Finally, if it can be shown that the distances obtained are Euclidean, we could use other methods that yield better predictions.
