# Distance-based methods for categorical sequences

Distance-based methods are a class of statistical techniques based on the use of distance, similarity or dissimilarity between data points defined by a distance or similarity function. The main idea behind these methods is to obtain a (pseudo)distance matrix in order to apply an unsupervised learning method such as

- clustering in different variations
- dimensionality reduction
- multidimensional scaling

as well as supervised learning methods, where we are interested in using the data to predict other variable for which we have observed or labeled values, for example: $k$-nearest neighbors ($k$-NN).

Distance-based methods allow application with a variety of data types, in particular, categorical sequences, i.e. sequences that take values in a finite set of categories or states and are indexed by time.

## Distance in categorical sequences

Several ways to compute distances between categorical sequences have been proposed in the context of natural language processing and bioinformatics. Particularly, a class of measures, known as _edit distances_, provide a quantification of the dissimilarity of a pair of sequences by counting the minimum number of operations required to obtain a sequence from the other. For instance, @Hamming-1950 proposed a distance for sequences of the same length that counts the number of positions with different states. @Levenshtein-1966 generalized the Hamming distance to sequences of different lengths by considering the minimum number of single character edits required, namely insertion, deletion and substitution. The Damerau-Levenshtein distance allows for an additional operation known as transposition or swaping of characters (@Damerau-1964). The Jaro-Winkler similarity measure counts the number of matches and transpositions but does not fulfill the triangle inequality (@Winkler-1990). Additionally, there are several algorithms to find the longest common subsequence from a pair of subsequences (see @Bergroth-2000), this could be seen as a measure that allows for insertion and deletion but not substitution nor transposition.

Also, note that from fundamental statistical methods for categorical data, considering the distribution of the states in a sequence, for a pair of sequences, the euclidean or $\chi^2$ distance can be obtained and used in this context.

## Optimal matching and applications

One generalization of the Levenshtein distance that allows for different substitution penalties for every pair of states is the Needleman-Wunsh algorithm, developed by @Needleman-Wunsch-1970 with the aim of comparing biological sequences (for example, DNA or protein sequences). This algorithm is an application of dynamic programming, an iterative method that simplifies an optimization problem by breaking it into a recursion of smaller problems that are simpler to solve. By choosing the optimal operation at each step, it is guaranteed that the overall solution is optimal as well. An adaptation of this algorithm, known as _optimal matching_ (OM), <!-- a sentence describing what the adapatation (generalization?) consists of would be perfect here, e.g. in which general arbitrary state transition costs are introduced --> was introduced in social sciences by @Abbott-1986 and has been widely applied in sociology, for instance, optimal matching has been employed in several studies tracking the professional development of specific groups of people, see @Chan-1995, @Torsten-2014 or @Gubler-2015, and to analyze life course data, for example, @Widmer-2009 or @Bastin-2015.

Limitations of optimal matching have been pointed out by some critics. A notable flaw raised by @Wu-2000 lies in the definition of the cost matrix — a core hyperparameter of the method — and the strong assumption about its symmetry as initially, the method considered substitution costs that were provided by an expert in the context. However, data-based aproaches for the calculation of the cost matrix have been proposed since then, see @Studer-2016.

Once a distance matrix has been obtained via optimal matching or any other technique that is applicable to categorical sequences, a distance-based method can be used depending on the specific interest of the research. Usually, clusters are obtained in order to identify common trajectories through visual inspection, @Abbott-1983 highlighted the possibility to use the distance matrix for multidimensional scaling, @Gabadinho-2013 proposed a way to identify typical patterns based on the coverage neighborhood of the sequences applied to childbirth histories, @Massoni-2009 combined optimal matching and self-organizing maps in the study of career path and employability. In addition, @Studer-2011 propose a methodology to analyze how covariates can explain the discrepancy between sequences based on their dissimilarities. However, to our knowledge, there has been no attempt at using categorical sequences for the prediction of other variables. 

In this work we are interested in studying the effect of different hyperparameters when using optimal matching to obtain pairwise distances of a group of sequences and subsequently employ them in distance-based methods. Particularly, we consider the effects on both clustering (unsupervised learning) and variable prediction with $k$-NN (supervised learning). For this purpose we analyze a new real-world dataset that includes categorical sequences, auxiliary information and other variables of interest: data from the "Women 40+ Healthy Aging Study".

## Women 40+ Healthy Aging Study

As part of the Women 40+ Healthy Aging Study <!-- any reference? -->, a large study that was conducted by the Department of Clinical Psychology and Psychotherapy of the University of Zurich, a psychometric instrument was developed in order to obtain information about the history of romantic relationships of women: the categorical sequence of interest.

The study was conducted between June 2017 and February 2018 with women between 40 and 75 years who (self-)reported good, very good or excellent health condition and the absence of acute or chronic somatic disease or mental disorder. The participants who reported psychotherapy or psychopharmacological treatment in the previous 6 months, as well as habitual drinkers, were excluded. Other exclusion criteria were pregnancy in the last 6 months, premature menopause, surgical menopause, intake of hormonal treatment (including contraceptives), shift-work and recent long-distance flight. The participants were recruited from the general population using online advertisement and flyers.

The questionnaire asked the participants to provide information about relationship phases starting from the age of 15 years until the current age at the time of the data collection. The phases were defined by the start and end age and for each phase and information about civil status, relationship status, living situation, children and quality of the relationship was collected. Before including the data corresponding to their own history, the participants were prompted to answer some of the questions based on an example. Some of the participants were excluded when the example entries were not correctly filled. After data cleaning and revisions for consistency the total number of individuals considered in this work is 239.

Additionally, personality scores for the women included in the study are available. Personality refers to the enduring characteristics and behavior that comprise the unique adjustment to life of a person, including major traits, interests, drives, values, self-concept, abilities, and emotional patterns. These scores are obtained via psychometric instruments and evaluate the main personality traits:

- Agreeableness
- Conscientiousness 
- Extraversion
- Neuroticism
- Openness

<!-- what follows here does not seem to flow properly. I think that what you want is to describe the analysis goals (in the applied context) and the analysis plan (so that it is connected) which then gives room to the subsections on each of the methods. Note that the k-nn part is also too long in comparison, and like the OM algorithm should probably be in its own section (e.g. clustering is not described in that much detail). My sketch looks like:

In this context we would like to identify common groups of individuals in order to characterize them and gain insight on "the social behaviour" of the study participants.

Additionally we would like to understand how the trajectories influence personality traits, i.e. we would like to understand the predective power of the trajectories over the traits.

For both of these goals we seek to apply distance-based methods employing the optimal matching. In order to cluster ... And to predict we ...

In what follows we describe the OM algo in detail, including hyperparameters: the cost matrix and normalization functions.
-->

In order to obtain groups of similar sequences, we apply a hierarchical agglomerative clustering analysis using Ward's method to minimize the dispersion within the clusters (@Murtagh-2014), for this purpose we use the method `"ward.D2"` of the `hclust` function in `R`. 

On the other hand, for prediction we use the function `k.nearest.neighbors` from the `R` package `FastKNN`. Given a training set $\mathcal{D} = {(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)}$ of $n$ labeled data points, where $x_i \in \mathbb{R}^d$ and $y_i \in \mathcal{Y}$, a finite set of class labels for classification or a continuous range of values for regression. $k$-NN provides a way to predict the label or value for a new, data point $x_{n+1}$ (for which $y_{n+1}$ is unknown) by finding the $k$ training data points closest to $x_{n+1}$ and taking a majority vote of their labels (for classification) or averaging the values of $Y$ (for regression). That is, for a given distance function $d(\cdot, \cdot)$ we predict $y_{n+1}$ as:

\begin{equation}
  \hat{y}_{n+1} = \frac{1}{k} \sum_{j=1}^k y_{(j)}
\end{equation}

where $j = {(1), \dots, (k)}$ index the nearest $k$ neighbors of $x_{n+1}$:

\begin{equation}
  d(x_{(1)} , x_{n+1}) < \dots < d(x_{(k)} , x_{n+1}) <         
  d(x_{(k+1)} , x_{n+1}) < \dots < d(x_{(n)} , x_{n+1})
\end{equation}

There are different choices for the distance function $d(\cdot, \cdot)$. For instance, the Euclidean or Mahalanobis distances are common choices. In our case, in order to analyze categorical sequences, we apply the more general OM algorithm to obtain a disimiliraty matrix.


## The OM algorithm {#om}

As mentioned above, optimal matching is a technique widely applied in social sciences for the comparison of categorical sequences. OM uses the Needleman-Wunsch algorithm to identify similarities between biological sequences that are usually represented as strings of characters.

The goal of OM is to find the best possible alignment between two sequences by considering the differences and equivalences between their elements and minimizing the total cost associated. The cost of changing between states of the sequences we are interested in aligning, can be defined in several ways including data-based methods or values supplied by experts in the particular field.

Consider a set of $n$ categorical states $S = \{s_1, \dots, s_n\}$, we define $X = (x_1, \dots, x_t)$, a sequence of length $t < \infty$, where $x_i \in S$ for $i = 1, \dots, t$. Further, let $\mathbf{S}$ be the set of all possible sequences with states belonging to $S$.

Now, let $X, Y \in \mathbf{S}$ be two sequences of size $t_X$ and $t_Y$, respectively. In order to numerically assess the disimilarity between the sequences $X$ and $Y$, we define an empty array $F$ of size $(t_X+1) \times (t_Y+1)$. Algorithm \@ref(OMalg) below shows the initialization and recursion to fill the array $F$.

\vspace{12pt}

\begin{algorithm}
\caption{Optimal matching.}
\label{OMalg}
\begin{algorithmic}[1]
\State $F(1, 1) \gets 0$
\For{$j \gets 2,t_Y+1$}
  \State $F(1,j) \gets F(1, j-1) + d$
\EndFor
\For{$i \gets 2,t_X+1$}
  \State $F(i,1) \gets F(i-1, 1) + d$
\EndFor
\For{$i \gets 2,t_X+1$}
  \For{$j \gets 2,t_Y+1$}
    \State $F(i,j) \gets \min\{F(i-1, j)+d, F(i, j-1)+d, F(i-1, j-1)+K(y_{i-1}, x_{j-1})\}$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

The value $d$ is the cost of inserting a gap in one of the sequences, also known as *indel* cost, and $K(y_{i-1}, x_{j-1})$ is the cost associated to change from the state $y_{i-1}$ to $x_{j-1}$, which is defined in a matrix $K$ of size $n \times n$, commonly known as the cost matrix.

Lines 1-7 of the OM algorithm correspond to initialization. Starting with a cost of 0 in $F(1, 1)$, the first row and column of $F$ represent cumulative costs of successively adding gaps. The remaining lines of the algorithm correspond to the row-wise recursion to fill the array $F$ according to the content of the sequences to be compared: at any step of the recursion, the algorithm is looking at a specific pair of indexes (location) and calculating if substitution or insertion/deletion is the cheapest operation. Successively adding the costs of the cheapest operations results in the overall optimal cost for aligning the sequences $X$ and $Y$.

In fact, when $F$ is completely filled, the value in the last cell, i.e. $F(t_X+1, t_Y+1)$ corresponds to the optimal cost of aligning the sequences $X$ and $Y$. It is possible to recover the steps that conduced to this alignment with a traceback from the last cell. However, this is not necessary to obtain the dissimilarities matrix for a set of sequences.


### Example 

Suppose that $S$ is the alphabet and let $X = \{S,E,N,D\}$ and $Y = \{A,N,D\}$ be two sequences in $\mathbf{S}$. 

Further let $d = 2$, and

$$
K(i,j) = 
\begin{cases}
0 & \text{if } i=j,\\
3 & \text{otherwise}
\end{cases}
$$

The array $F$ is initialized as follows:

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 |   |   |   |   |
| N | 4 |   |   |   |   |
| D | 6 |   |   |   |   |


Then, to fill the second row of $F$ we proceed as follows:

\begin{align*}
F(2,2) &= \min\{F(1, 2)+d, F(2, 1)+d, F(1, 1)+k(y_{1}, x_{1})\} \\
       &= \min\{2+2, 2+2, 0+3\} \\
       &= 3 \\
F(2,3) &= \min\{F(1, 3)+d, F(2, 2)+d, F(1, 2)+k(y_{1}, x_{2})\} \\
       &= \min\{4+2, 3+2, 2+3\} \\
       &= 5 \\
F(2,4) &= \min\{F(1, 4)+d, F(2, 3)+d, F(1, 3)+k(y_{1}, x_{3})\} \\
       &= \min\{6+2, 5+2, 4+3\} \\
       &= 7 \\
F(2,5) &= \min\{F(1, 5)+d, F(2, 4)+d, F(1, 4)+k(y_{1}, x_{4})\} \\
       &= \min\{8+2, 7+2, 6+3\} \\
       &= 9
\end{align*}

What yields:

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 | 3 | 5 | 7 | 9 |
| N | 4 |   |   |   |   |
| D | 6 |   |   |   |   |

Finally, after completing the recursion for the remaining rows, we obtain the following $F$ array:

|   |   | S | E | N | D |
|---|---|---|---|---|---|
|   | 0 | 2 | 4 | 6 | 8 |
| A | 2 | 3 | 5 | 7 | 9 |
| N | 4 | 5 | 6 | 5 | 7 |
| D | 6 | 7 | 8 | 7 | 5 |

In this simple example, we can easily obtain two optimal (equivalent) alignments without using the algorithm:

S E N D with  

A &ndash; N D or  

&ndash; A N D  
<br>
In both cases we have two matches (cost 0), one mismatch (cost 3) and one gap (cost 2), giving a total cost 5 that is exactly what we obtained in the last cell of $F$.

The cost of inserting a gap ($d$) is also known as *indel* (insert or delete) cost. In this example we can observe that, in order to obtain sequence $X$ from $Y$ we have to **insert** a term (i.e. insert a gap and then change its value to a specific state). Equivalently, to obtain sequence $Y$ starting from $X$ we have to **delete** one term.

The `R` packages `TraMineR` (@TraMineR) and `TraMineRextras` provide several functions to define, analyze and visualize sequential data. In particular, `TraMineR` implements the OM algorithm and offers several methods for computing the cost matrix $K$ and the normalization of the dissimilarity matrix.


## Cost matrix

The cost matrix $K$ is a symmetric matrix of size $n \times n$. The value in the $i$-th row and $j$-th column $K(s_i, s_j)$ indicates the cost of moving from state $s_i$ in time $t > 0$ to state $s_j$ in $t+1$.

The following are the methods available in `TraMineR` to obtain the cost matrix.

### Transition rates (`TRATE`):


The substitution cost between states $s_i$ and $s_j$, $1 \leq i, j \leq n$ is based on the observed frequencies of the transitions between the states and is calculated as:

\begin{equation}
\label{eq:transition}
K(s_i, s_j) = c - P(s_i|s_j) - P(s_j|s_i),
\end{equation}

where $P(s_i|s_j)$ is the probability of transition from state $s_j$ in time $t$ to $s_i$ in time $t+1$ and $c$ is a constant, set to a value such that $0 \leq K(s_i, s_j) \leq 2$).

The implementation of this method uses a default value of $c=2$ which results in substitution costs that are close to 2. Additionally, as pointed out by @Studer-2016, the calculation of substitution costs can lead to violations of the triangle inequality, meaning that we obtain a dissimilarity instead of a distance measure.


### Chi-squared distance (`FUTURE`):

The $\chi^2$-distance is a weighted sum of the squared differences of distribution vector frequencies. The weight is given by the inverse of the proportion of the total time spent in the state, meaning that the differences on rare states have higher weights. 

\begin{align}
\label{eq:chisq}
K(s_i, s_j) &= d_{\chi^2}(\mathbf{P_i}, \mathbf{P_j}) \\
            &= \left[ \sum_{l = 1}^n \alpha_l^{-1} \left( P(s_l|s_i) - P(s_l|s_i) \right)^2 \right]^{1/2}
\end{align}

where $\mathbf{P_.} = (P(s_1|s_.), \dots, P(s_n|s_.))'$, $\alpha_l = \sum_{h=1}^n P(s_l|s_n)$ and $i \neq j$.

It has been shown via simulation, that this distance is particularly sensitive to the time spent in each state but not so much to the order of the states in a sequence (see @Studer-2016).


### Relative frequencies (`INDELS` and `INDELSLOG`):

\begin{equation}
\label{eq:indels_cost}
K(s_i, s_j) = d_i + d_j,
\end{equation}

where the *indel* cost $d_i$ depends on the state and takes values: 

\begin{align}
g_i &= \frac{1}{f_i}, &\text{for method `INDEL`}, \\
g_i &= \log\left({\frac{2}{1+f_i}}\right), &\text{for method `INDELSLOG`}
\end{align}

and $f_i$ is the relative frequency of the state $s_i$ for $i = 1, \dots, n$. 

#### Remarks: 
- For methods `TRATE` and `FUTURE`, the unique *indel* value is $d = max_{1 \leq i,j \leq n} K(i,j)/2$, so that the cost of any change of state is always lower or equal than deleting and inserting an element (or vice versa). The reason behind is that higher *indel* costs, compared to the substitution costs, produce dissimilarities that are greatly affected by time shifts.
- The Needleman-Wunsch algorithm with constant costs for mismatch is known as Levenshtein distance (@Levenshtein-1966), a string metric widely used in computer science.
- In general, the resulting measure of the algorithm is a dissimilarity. However, if the cost matrix fulfills the triangle inequality, we obtain a distance measure (@Yujian-2007).


## Normalization

By design, OM can deal with sequences of different lengths via insertions. However, in cases when the lengths of the sequences differ greatly, it can be useful to account for this differences with a normalization factor. 

Given a set two sequences $X, Y \in \mathbf{S}$ of length $t_X$ and $t_Y$, respectively. Let $d(X,Y)$ be the dissimilarity between the sequences $X$ and $Y$, $t_{max}$ the length of the longest sequence in $\mathbf{S}$ and $d_{max}$ the maximum dissimilarity between any pair of sequences in $\mathbf{S}$.

`TraMineR` offers the following options to normalize the dissimilarities between sequences:

- `maxlength`:
$$\frac{d(X,Y)}{t_{max}}$$
- `gmean`: 
$$1- \frac{d_{max}-d(X,Y)}{\sqrt{t_X*t_Y}}$$
- `maxdist`: 
$$\frac{d(X,Y)}{d_{max}}$$
